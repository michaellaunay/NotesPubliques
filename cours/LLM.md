# Large Language Models

Ressources : 
La recherche en France :
[IA au Loria, laboratoire du CNRS](https://ia.loria.fr/portfolio/)
[LLM(ChatGPT) - D√©-coder les grands mod√®les de langage - Christophe Cerisara | Codeurs en Seine](https://www.youtube.com/@Codeursenseine)

[Aper√ßu complet sur les LLM et l'ing√©nierie des prompts | Baamtu](https://youtu.be/MWotDXpD6SI?si=txySpWEWK4vQ2A6V)

[The Practical Guides for Large Language Models | LLMsPracticalGuide](https://github.com/Mooler0410/LLMsPracticalGuide)

https://machinelearningmastery.com

[Les √©l√®ves utilisent ChatGPT pour leurs devoirs et les enseignants utilisent ChatGPT pour les corriger, d'apr√®s des rapports | Developpez.com](https://intelligence-artificielle.developpez.com/actu/355048/Les-eleves-utilisent-ChatGPT-pour-leurs-devoirs-et-les-enseignants-utilisent-ChatGPT-pour-les-corriger-d-apres-des-rapports-qui-suscitent-des-comparaisons-avec-les-examens-ecrits-et-oraux/)  
  
[Comment utiliser un LLM open source ? | Octo.com](https://blog.octo.com/comment-utiliser-un-llm-open-source-1)  
  
[Les mod√®les de langage peuvent contenir des backdoors | Nextinpact](https://next.ink/123823/les-modeles-de-langage-peuvent-contenir-des-backdoors/)

[Tuner un LLM √† partir de la documentation d'un projet](https://youtu.be/Ivp5PGIbGMw)

[n8n - Workflow Automation](https://github.com/n8n-io)

[Unslow AI training & finetuning Get 30x faster with unsloth](https://unsloth.ai/)

[Votre LLM (ChatGPT-like) √† la maison et comment coder par dessus. | Korben](https://youtu.be/1aXPuFrPtr0)

[Un √©quivalent de GitHub Copilot gratuit √† installer sur Visual Studio Code | Korben](https://youtu.be/6a5GHdoa8OM) [TabbyML](https://github.com/TabbyML/tabby) √âcrit en RUST, Permet d'installer un serveur conversationnel via docker. Le client pour visual studio code est disponible via les extensions en cherchant tabby. Tabby permet de g√©rer des √©quipes et poss√®de une API

Le futur de l'IDE **Cursor** [Using Cursor - the AI powered VS Code alt for the first time‚Ä¶| Huw prosser](https://youtu.be/n4DRPGWTmpc)

Sur Hugging Face il y a plus de 70 000 Mod√®les open source [Huggingface/models ](https://huggingface.co/models)
Aujourd'hui l'apprentissage d'un LLM se fait sur 1000 000 000 000 de mots. L'architerture des transformers (2018 Google) est celle de la quasi totalit√© des LLM, elle vise √† pr√©dire le mot suivant.

Les LLM sont tr√®s bon pour √©crire du code simple, faire du code compliqu√© n√©cessite de le guider et donc d'avoir une interaction avec.

Pour g√©rer la complexit√© il y a des tentatives d'agents de models de langage (orchestration de diff√©rents LLMs) (Langchain, Coala).

Aujourd'hui on constate les capacit√©s des LLMs mais on commence √† peine √† pouvoir les expliqu√©s (presque pas, embryons de th√©ories).
Les observations :
- M√©morisation, compression, structuration et g√©n√©ralisation
- Capacit√©s √©mergentes :
	- "In context learning" Capacit√© de g√©n√©raliser (seuls les LLM font cela), mais n'√©merge qu'√† partir de 10e10 de param√®tres.
	- Capacit√© de faire des additions de 3 chiffres (10e10).
	- R√©pondre √† des questions.
	- G√©n√©rer des programmes.
	- Jason Wei a d√©nombr√© 137 capacit√©s √©mergentes :
		- D√©composition d'un probl√®me en √©tapes.
		- "Prompt of thought" (Acc√©der au raisonnement du LLM)
		- "Analogical prompting"  (Faire √©laborer au LLM la d√©marche √† appliquer avant de l'appliquer)
		- Instruction proc√©durale
		- Anagramme
		- Arithm√©tique modulaire
		- Probl√®mes simples de math
		- D√©duction logique
		- D√©duction analytique
		- Th√©orie de l'esprit (avoir un mod√®le de son locuteur) ?
		- ...
Embryons de th√©ories **Passage √† l'√©chelle**("scaling law") :
	- Lois de Chinchilla (On mesure les capacit√© du mod√®le lorsqu'il devient de plus en plus gros Baidu 2017 puis Google 2018 : l'apprentissage suit des lois qui sont fonction du nombre de donn√©es, et aboutisse en 2022 √† Chinchilla, c'est loi sont en `L(N)=A/N**ùõº` o√π ùõº est diff√©rent pour chacune des architectures avec le meilleur score de 0,54 pour les "transformers")
	- Grokking (Apprend bestialement puis apr√®s beaucoup de donn√©es g√©n√©ralise)
	- Double descente
	- Transition de phases

# Le deeplearning un sous ensemble de l'IA
# Les mod√®les de langage
- 1990-2000 Les n-grams
- 2013 word2vec (Thomas Mikolov Google)
- 2015 Les RNN
- 2017 Les transformers (Vaswani et al) ([Attention Is All You Need](https://arxiv.org/abs/1706.03762))
# L'arbre des LLMs
La grande majorit√© des mod√®les actuels d√©rivent de Word2Vec
https://raw.githubusercontent.com/Mooler0410/LLMsPracticalGuide/main/imgs/tree.jpg

Des LLM opensources d√©di√©s au code CodeLama, CodeGen.
# 1. Introduction aux LLMs

# 2. Fondations Th√©oriques

## 2.1 Traitement du langage naturel (TLP) 
### 2.1.1 Introduction au TLP
- D√©finition et objectifs du TLP.
- Historique du TLP: de la r√®gle bas√©e √† l'apprentissage automatique.

### 2.1.2 Niveaux de traitement en TLP
- Traitement lexical, syntaxique, s√©mantique, et pragmatique.
- Analyse morphologique et lemmatisation.
- Analyse syntaxique: arbres de d√©pendance et de constituance.
- Repr√©sentation s√©mantique et compr√©hension de la langue.

### 2.1.3 Les outils et ressources en TLP
- Corpus de texte, lexiques, et bases de donn√©es linguistiques.
- Logiciels et biblioth√®ques de TLP (NLTK, spaCy, Stanford NLP).

## 2.2 Mod√®les de langage statistiques vs. bas√©s sur le deep learning

### 2.2.1 Mod√®les de langage statistiques
- Mod√®les N-grammes et leurs limites.
- Techniques de lissage et d'√©valuation des mod√®les statistiques.

#### 2.5 Mod√®les de langage bas√©s sur le deep learning
- Introduction aux r√©seaux de neurones en TLP.
- Architectures de mod√®le: RNN, LSTM, GRU.

#### 2.6 Transition vers le deep learning
- Les d√©fis des mod√®les statistiques et la mont√©e du deep learning.
- Avantages des mod√®les bas√©s sur le deep learning en termes de contextualisation et performance.

### Activit√©s en classe
- Comparaison de la performance d'un mod√®le N-gramme et d'un mod√®le LSTM sur une t√¢che de compl√©tion de texte.

### Devoir
- Construire un simple mod√®le N-gramme et un mod√®le RNN pour la g√©n√©ration de texte et comparer les r√©sultats.

---

## Les r√©seaux de neurones et l'apprentissage profond en TLP

### Objectif de la le√ßon
- Fournir une compr√©hension des r√©seaux de neurones et de l'apprentissage profond.
- Discuter de l'application de ces concepts en TLP.

### Contenu
#### 2.7 Fondements des r√©seaux de neurones
- Neurones artificiels et topologies de r√©seau.
- Processus d'apprentissage et r√©tropropagation.

#### 2.8 R√©seaux de neurones r√©currents (RNN)
- Particularit√©s et utilisation des RNN en TLP.
- Probl√®mes des RNN et introduction des LSTM et GRU.

#### 2.9 L'essor des architectures Transformer
- Introduction aux Transformers et √† l'auto-attention.
- BERT, GPT, et autres mod√®les bas√©s sur Transformer.

### Activit√©s en classe
- Impl√©mentation et entra√Ænement d'un mod√®le LSTM pour la classification des sentiments.

### Devoir
- Lecture critique d'un article de recherche sur les architectures Transformer et leur impact sur le TLP.

---

Ce plan de cours est con√ßu pour √™tre progressif, en commen√ßant par les fondamentaux du TLP et en avan√ßant vers des concepts plus complexes comme l'apprentissage profond. Les activit√©s pratiques et les devoirs sont destin√©s √† renforcer la th√©orie et √† donner aux √©tudiants une exp√©rience pratique avec les techniques modernes du TLP.