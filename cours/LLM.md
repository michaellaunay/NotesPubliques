# Large Language Models

# Le deeplearning un sous ensemble de l'IA
# Les mod√®les de langage
- 1990-2000 Les n-grams
- 2013 word2vec (Thomas Mikolov Google)
- 2015 Les RNN
- 2017 Les transformers (Vaswani et al) ([Attention Is All You Need](https://arxiv.org/abs/1706.03762))
# L'arbre des LLMs
La grande majorit√© des mod√®les actuels d√©rivent de Word2Vec
https://raw.githubusercontent.com/Mooler0410/LLMsPracticalGuide/main/imgs/tree.jpg

Des LLM opensources d√©di√©s au code CodeLama, CodeGen.
# 1. Introduction aux Mod√®les de Langage √† Grande √âchelle (LLM)

Dans le domaine en rapide √©volution de l'intelligence artificielle (IA), les mod√®les de langage √† grande √©chelle (LLM) repr√©sentent une avanc√©e significative, marquant une √®re nouvelle dans la fa√ßon dont les machines comprennent et g√©n√®rent le langage humain. Issus d'un sous-ensemble de l'IA connu sous le nom de deep learning, les LLMs sont au c≈ìur des innovations technologiques actuelles, transformant les interactions humain-machine, la programmation, l'√©ducation, et bien plus encore.

## 1.1. Origines et √âvolution

Les fondements des LLMs remontent aux ann√©es 1990 et 2000 avec l'av√®nement des n-grams, suivis par l'introduction de Word2Vec par Thomas Mikolov chez Google en 2013. Cette √©volution a continu√© avec les RNN en 2015 et a √©t√© r√©volutionn√©e par les transformers en 2017, un mod√®le introduit par Vaswani et al. dans leur papier "Attention Is All You Need". Ces transformers sont la base de la quasi-totalit√© des LLM actuels, permettant de pr√©dire le mot suivant dans une s√©quence avec une pr√©cision sans pr√©c√©dent.

## 1.2. Capacit√©s et Applications

Les LLMs, avec leur architecture transformer, ont d√©montr√© une gamme √©tonnante de capacit√©s allant de la simple g√©n√©ration de texte √† la r√©solution de probl√®mes complexes de programmation. Des plateformes comme Hugging Face h√©bergent aujourd'hui plus de 70 000 mod√®les open source, t√©moignant de l'√©norme int√©r√™t et de la variabilit√© des applications possibles. Ces mod√®les ont trouv√© leur place non seulement dans la r√©daction de code mais √©galement dans l'√©ducation, o√π √©l√®ves et enseignants utilisent des outils comme ChatGPT pour les devoirs et la correction, illustrant leur potentiel disruptif et polyvalent.

## 1.3. D√©fis et Perspectives

Toutefois, les LLMs ne sont pas sans d√©fis. La question de la s√©curit√©, avec la possibilit√© que les mod√®les contiennent des backdoors, ainsi que les pr√©occupations √©thiques autour de leur utilisation, n√©cessitent une attention soutenue. Par ailleurs, les efforts pour comprendre et expliquer les m√©canismes sous-jacents des LLMs ne font que commencer, avec des embryons de th√©ories comme les lois de Chinchilla et le ph√©nom√®ne de Grokking qui tentent de cartographier le comportement de ces mod√®les √† mesure qu'ils √©voluent.

## Perspectives

L'avenir des LLMs s'annonce riche en innovations et en d√©fis. Alors que nous commen√ßons tout juste √† gratter la surface de leurs capacit√©s, leur int√©gration croissante dans notre quotidien et leur impact potentiel sur divers secteurs promettent de red√©finir notre relation avec la technologie.
Cette technologie, comme tous les outils, apporte avec elle son lot de risques, tant pour la s√©curit√© des donn√©es et la v√©racit√© des informations, que pour la robustesse des infrastructures, sans oublier l'impact de sa consommation √©nerg√©tique. Elle n√©cessite des produits complexes qui demandent des ressources rares, dont l'extraction est polluante. Ainsi, elle porte en elle √† la fois des promesses et des menaces. Il nous appartient de l'utiliser pour le meilleur.

# 2. Fondations Th√©oriques

## 2.1 Traitement du langage naturel (TLP) 
### 2.1.1 Introduction au TLP

Le Traitement du Langage Naturel (TLP), ou Natural Language Processing (NLP) en anglais, se d√©finit comme la branche de l'intelligence artificielle qui se concentre sur l'interaction entre les ordinateurs et le langage humain. Son objectif principal est de permettre aux machines de comprendre, interpr√©ter, et produire du langage humain d'une mani√®re qui soit √† la fois significative et utile. Ceci inclut une gamme de t√¢ches telles que la traduction automatique, la reconnaissance vocale, et la g√©n√©ration de texte.

Historiquement, le TLP a commenc√© avec des approches bas√©es sur des r√®gles, o√π les linguistes formulaient des r√®gles grammaticales et syntaxiques pour analyser et comprendre le langage. Cette m√©thode n√©cessitait une connaissance approfondie de la langue concern√©e et √©tait extr√™mement laborieuse √† mettre en ≈ìuvre. En outre, elle manquait de flexibilit√© et ne pouvait pas g√©rer efficacement les nuances et la variabilit√© inh√©rentes au langage humain.

Avec l'av√®nement de l'informatique et la disponibilit√© croissante de grandes quantit√©s de donn√©es textuelles, le champ du TLP a progressivement √©volu√© vers des m√©thodes d'apprentissage automatique. Ces approches, contrairement aux syst√®mes bas√©s sur des r√®gles, apprennent √† partir de vastes ensembles de donn√©es, permettant aux mod√®les de s'adapter et de reconna√Ætre des sch√©mas linguistiques complexes sans une programmation explicite pour chaque cas. Cette transition a marqu√© un tournant dans le domaine du TLP, ouvrant la voie √† des progr√®s significatifs et √† la cr√©ation de syst√®mes beaucoup plus performants et flexibles.

### 2.1.2 Niveaux de traitement en TLP

Le traitement du langage naturel (TLP) op√®re √† plusieurs niveaux pour analyser et comprendre le langage humain, chacun ajoutant une couche de complexit√© et de nuance √† l'interpr√©tation du texte. Voici un aper√ßu des principaux niveaux de traitement en TLP :

- **Traitement lexical** : Ce niveau concerne l'analyse des mots individuels hors contexte. Il englobe l'identification des types de mots (noms, verbes, adjectifs, etc.), ainsi que la reconnaissance des entit√©s nomm√©es (noms de personnes, lieux, organisations). Le traitement lexical inclut √©galement l'identification de la racine des mots pour faciliter leur analyse.

- **Analyse morphologique et lemmatisation** : L'analyse morphologique s'int√©resse √† la structure interne des mots, identifiant les racines, les pr√©fixes, les suffixes, et la formation des mots. La lemmatisation, quant √† elle, consiste √† ramener un mot √† sa forme de base ou de dictionnaire, ce qui est crucial pour r√©duire la complexit√© et augmenter la g√©n√©ralit√© de l'analyse.

- **Analyse syntaxique** : Ce niveau se concentre sur la structure grammaticale des phrases, d√©terminant la mani√®re dont les mots sont organis√©s pour former des phrases coh√©rentes. L'analyse syntaxique peut √™tre r√©alis√©e √† travers les arbres de d√©pendance, qui montrent les relations entre les mots d'une phrase, ou les arbres de constituance, qui d√©composent la phrase en sous-unit√©s (phrases, groupes nominaux, etc.).

- **Traitement s√©mantique** : Ici, le but est de comprendre la signification des mots dans leur contexte sp√©cifique. La repr√©sentation s√©mantique implique l'attribution de sens aux phrases et aux textes, allant au-del√† de la structure grammaticale pour saisir les nuances de signification, les ambigu√Øt√©s, et les relations s√©mantiques entre les entit√©s et les concepts.

- **Traitement pragmatique** : Le niveau pragmatique s'attache √† l'usage et au contexte du langage, consid√©rant comment le sens est construit dans des situations de communication r√©elles. Cela inclut l'analyse du discours, la reconnaissance de l'ironie ou du sarcasme, et la compr√©hension de l'intention communicative derri√®re les √©nonc√©s.

Chaque niveau de traitement contribue √† une compr√©hension plus profonde et plus nuanc√©e du texte, permettant aux syst√®mes de TLP de r√©pondre et d'interagir de mani√®re plus naturelle et intelligente.

### 2.1.3 Les outils et ressources en TLP

Pour mener √† bien les recherches et les projets en Traitement du Langage Naturel (TLP), divers outils et ressources sont indispensables. Ces ressources permettent d'analyser, de comprendre, et de g√©n√©rer le langage humain de mani√®re efficace. Voici une vue d'ensemble des principaux outils et ressources disponibles dans le domaine :

- **Corpus de texte** : Les corpus de texte sont des ensembles de documents √©crits qui servent de mat√©riel de base pour l'entra√Ænement et le test des mod√®les de TLP. Ils peuvent √™tre g√©n√©ralistes, couvrant un large √©ventail de sujets, ou sp√©cialis√©s dans des domaines sp√©cifiques (m√©dical, juridique, litt√©raire, etc.). Les corpus sont essentiels pour d√©velopper des mod√®les de langage et pour l'√©valuation de leur performance.

- **Lexiques et bases de donn√©es linguistiques** : Les lexiques sont des collections de mots ou de phrases avec des informations associ√©es (d√©finitions, synonymes, antonymes, etc.). Les bases de donn√©es linguistiques, telles que WordNet, fournissent des structures relationnelles entre les mots, facilitant l'analyse s√©mantique et le traitement du sens. Ces ressources sont cruciales pour la lemmatisation, la reconnaissance d'entit√©s nomm√©es, et d'autres t√¢ches de TLP.

- **Logiciels et biblioth√®ques de TLP** :
  - **NLTK (Natural Language Toolkit)** : Une des biblioth√®ques les plus utilis√©es pour le travail en TLP. Elle offre des outils pour le traitement de texte, l'analyse syntaxique, la reconnaissance d'entit√©s nomm√©es, et plus encore, rendant le TLP accessible aux d√©butants comme aux chercheurs exp√©riment√©s.
  - **spaCy** : Une biblioth√®que moderne et rapide pour le TLP en Python, con√ßue pour la production. Elle est particuli√®rement reconnue pour son efficacit√© dans le traitement et l'analyse de grands volumes de texte.
  - **Stanford NLP** : Un ensemble d'outils d√©velopp√© par le Stanford Natural Language Processing Group. Il inclut une vari√©t√© de mod√®les de TLP et d'algorithmes d'analyse linguistique pour le traitement syntaxique, la reconnaissance d'entit√©s nomm√©es, et l'analyse s√©mantique.

Ces outils et ressources jouent un r√¥le fondamental dans le d√©veloppement et l'impl√©mentation de solutions de TLP. Ils permettent aux chercheurs et aux d√©veloppeurs de construire des applications sophistiqu√©es de traitement du langage, allant de la traduction automatique √† l'analyse de sentiments, en passant par les assistants vocaux intelligents.

## 2.2 Mod√®les de langage statistiques vs. bas√©s sur le deep learning

### 2.2.1 Mod√®les de langage statistiques

Les mod√®les de langage statistiques ont jou√© un r√¥le fondamental dans les premiers d√©veloppements du traitement du langage naturel (TLP), en permettant aux ordinateurs de traiter et de comprendre le langage humain √† partir de l'analyse statistique de grandes quantit√©s de donn√©es textuelles.

- **Mod√®les N-grammes** : Au c≈ìur de ces mod√®les statistiques se trouvent les mod√®les N-grammes, qui pr√©disent le mot suivant dans une s√©quence en se basant sur les \(N-1\) mots pr√©c√©dents. Par exemple, dans un mod√®le trigramme (\(N=3\)), la probabilit√© du mot suivant est estim√©e en fonction des deux mots qui le pr√©c√®dent. Ces mod√®les sont simples mais √©tonnamment efficaces pour de nombreuses t√¢ches de TLP. Cependant, ils ont des limites significatives, notamment leur incapacit√© √† capturer des d√©pendances √† long terme dans le texte et leur tendance √† gonfler l'espace de stockage n√©cessaire, car la taille du mod√®le cro√Æt exponentiellement avec la valeur de \(N\).

- **Limites des mod√®les N-grammes** : Outre les d√©fis li√©s √† l'espace de stockage et aux d√©pendances √† long terme, les mod√®les N-grammes luttent √©galement contre le probl√®me des mots hors vocabulaire (OOV) et ne peuvent pas g√©rer efficacement la polys√©mie (mots ayant plusieurs significations) ou la synonymie (mots diff√©rents ayant des significations similaires).

- **Techniques de lissage** : Pour att√©nuer certains de ces probl√®mes, les techniques de lissage ont √©t√© d√©velopp√©es. Ces m√©thodes ajustent les probabilit√©s des s√©quences de mots pour √©viter les probabilit√©s nulles pour les s√©quences non observ√©es dans le corpus d'entra√Ænement. Les techniques courantes incluent le lissage de Laplace et le lissage de Good-Turing, qui r√©allouent certaines des probabilit√©s des √©v√©nements observ√©s vers les √©v√©nements non observ√©s.

- **√âvaluation des mod√®les statistiques** : L'√©valuation de ces mod√®les se fait typiquement √† l'aide de mesures telles que la perplexit√©, qui mesure √† quel point un mod√®le est "surpris" par de nouvelles donn√©es. Une perplexit√© plus faible indique un mod√®le qui pr√©dit mieux les s√©quences de mots. Cependant, malgr√© leur utilit√©, les limites intrins√®ques des mod√®les N-grammes et les d√©fis li√©s √† leur √©valuation ont men√© √† la recherche de nouvelles approches, notamment celle des mod√®les bas√©s sur le deep learning, qui offrent une capacit√© sup√©rieure √† mod√©liser des d√©pendances complexes et √† long terme dans le langage.

### 2.2.2 Mod√®les de langage bas√©s sur le deep learning

Avec l'avancement de la technologie et la qu√™te pour surmonter les limites des mod√®les statistiques, le domaine du traitement du langage naturel (TLP) a assist√© √† une r√©volution majeure gr√¢ce √† l'adoption du deep learning. Cette transition a ouvert la voie √† des mod√®les de langage beaucoup plus puissants et flexibles, capables de comprendre et de g√©n√©rer du langage avec une pr√©cision sans pr√©c√©dent.

- **Introduction aux r√©seaux de neurones en TLP** : Les r√©seaux de neurones artificiels s'inspirent du fonctionnement du cerveau humain et sont compos√©s de couches de neurones artificiels. En TLP, ils permettent de mod√©liser des s√©quences de mots et de capturer les relations s√©mantiques complexes entre elles. Contrairement aux mod√®les statistiques, les r√©seaux de neurones sont capables de g√©rer des d√©pendances √† long terme et d'apprendre des repr√©sentations riches et hi√©rarchis√©es du langage.

- **Architectures de mod√®le** :
  - **RNN (R√©seaux de Neurones R√©currents)** : Les RNN sont sp√©cialement con√ßus pour traiter des s√©quences de donn√©es, comme le texte, en traitant chaque √©l√©ment de la s√©quence un par un tout en maintenant une m√©moire (√©tat cach√©) de ce qui a √©t√© trait√© jusqu'√† pr√©sent. Cela leur permet de capturer des informations contextuelles dans une s√©quence. Cependant, les RNN standards luttent contre le probl√®me de la disparition ou de l'explosion des gradients, rendant difficile l'apprentissage de d√©pendances √† long terme.
  
  - **LSTM (Long Short-Term Memory)** : Les LSTM sont une variante des RNN qui introduisent des portes (input, forget, et output gates) pour r√©guler le flux d'informations. Elles permettent au mod√®le de retenir ou d'oublier des informations de mani√®re s√©lective, facilitant ainsi l'apprentissage de d√©pendances √† long terme sans succomber aux probl√®mes des gradients disparus ou explosifs.
  
  - **GRU (Gated Recurrent Unit)** : Les GRU sont une autre variante des RNN qui simplifient l'architecture LSTM tout en conservant sa capacit√© √† apprendre des d√©pendances √† long terme. Avec seulement deux portes (reset et update gates), les GRU offrent souvent une efficacit√© comparable aux LSTM mais avec une complexit√© moindre, ce qui peut conduire √† une r√©duction du temps d'entra√Ænement et des besoins en ressources.

Ces architectures de deep learning ont significativement am√©lior√© la performance des mod√®les de langage, permettant des avanc√©es majeures dans des applications de TLP telles que la traduction automatique, la g√©n√©ration de texte, et la compr√©hension de la langue naturelle. Leur capacit√© √† apprendre des repr√©sentations profondes du langage a marqu√© une √©tape importante dans l'√©volution des technologies de traitement du langage.

### 2.2.3 Transition vers le deep learning

La transition des mod√®les de langage statistiques vers ceux bas√©s sur le deep learning marque une √©volution cruciale dans le domaine du traitement du langage naturel (TLP). Cette mutation s'est op√©r√©e en r√©ponse aux d√©fis inh√©rents aux approches statistiques et a √©t√© facilit√©e par les avanc√©es technologiques et l'augmentation de la puissance de calcul disponible.

- **Les d√©fis des mod√®les statistiques** : Malgr√© leur utilit√© dans les premiers stades du d√©veloppement du TLP, les mod√®les statistiques, notamment les mod√®les N-grammes, pr√©sentent plusieurs limitations. Leur capacit√© √† capturer des relations complexes et √† long terme entre les √©l√©ments du langage est restreinte, ce qui les rend moins efficaces pour comprendre le contexte ou la nuance des s√©quences de mots. De plus, ils sont souvent incapables de g√©rer de mani√®re efficace les mots rares ou inconnus, limitant ainsi leur applicabilit√© √† de nouveaux domaines ou vocabulaires.

- **La mont√©e du deep learning** : Face √† ces d√©fis, le deep learning a √©merg√© comme une solution puissante, offrant une approche fondamentalement diff√©rente pour mod√©liser le langage. Les architectures de r√©seaux de neurones, telles que les RNN, LSTM, et GRU, ont permis de surmonter bon nombre des limitations des mod√®les statistiques en apprenant des repr√©sentations profondes et hi√©rarchiques du langage √† partir de donn√©es. Cette capacit√© √† apprendre de mani√®re endog√®ne √† partir de grandes quantit√©s de texte a r√©volutionn√© le TLP.

- **Avantages des mod√®les bas√©s sur le deep learning** :
  - **Contextualisation** : Les mod√®les de deep learning excellent dans la compr√©hension du contexte, capable de saisir des nuances s√©mantiques fines et des d√©pendances √† long terme dans le texte. Cette profondeur de compr√©hension permet des applications plus sophistiqu√©es et naturelles du TLP, telles que la compr√©hension du langage naturel (NLU) et la r√©ponse aux questions (QA).
  
  - **Performance** : En termes de performance brute, les mod√®les de deep learning surpassent largement les approches statistiques traditionnelles sur une multitude de t√¢ches de TLP. Que ce soit dans la traduction automatique, la classification de texte, ou la g√©n√©ration de langage, les mod√®les bas√©s sur le deep learning √©tablissent r√©guli√®rement de nouveaux standards de performance.

La transition vers le deep learning a non seulement adress√© les limitations des mod√®les statistiques mais a √©galement ouvert de nouvelles voies d'exploration et d'innovation dans le TLP. Cette avanc√©e a conduit √† une am√©lioration significative des capacit√©s des syst√®mes de TLP, rendant possible des interactions homme-machine plus naturelles et intelligentes.

### 2.3. Activit√©s en classe
- Comparaison de la performance d'un mod√®le N-gramme et d'un mod√®le LSTM sur une t√¢che de compl√©tion de texte.

### 2.4. Devoir
- Construire un simple mod√®le N-gramme et un mod√®le RNN pour la g√©n√©ration de texte et comparer les r√©sultats.
# 3. Les r√©seaux de neurones et l'apprentissage profond en TLP

## Objectif
- Fournir une compr√©hension des r√©seaux de neurones et de l'apprentissage profond.
- Discuter de l'application de ces concepts en TLP.

## 3.1. Fondements des r√©seaux de neurones

La compr√©hension des r√©seaux de neurones et de l'apprentissage profond constitue une pierre angulaire du traitement du langage naturel (TLP) moderne. Ces concepts empruntent √† la neurologie et √† l'informatique pour cr√©er des mod√®les capables de traiter le langage humain de mani√®re in√©dite.

- **Neurones artificiels et topologies de r√©seau** : Les neurones artificiels sont les unit√©s de base des r√©seaux de neurones, inspir√©s par les neurones biologiques du cerveau humain. Chaque neurone re√ßoit des entr√©es, les traite √† l'aide d'une fonction d'activation (comme la sigmo√Øde, ReLU), et transmet le r√©sultat aux neurones suivants. Les r√©seaux de neurones sont organis√©s en couches, comprenant g√©n√©ralement une couche d'entr√©e, une ou plusieurs couches cach√©es, et une couche de sortie. La topologie d'un r√©seau, c'est-√†-dire l'arrangement de ces neurones et couches, influe directement sur sa capacit√© √† apprendre et √† g√©n√©raliser √† partir des donn√©es.

- **Processus d'apprentissage et r√©tropropagation** : L'apprentissage dans les r√©seaux de neurones se fait √† travers un processus appel√© r√©tropropagation (backpropagation). Durant cet apprentissage, le r√©seau ajuste les poids de ses connexions pour minimiser la diff√©rence entre la sortie pr√©dite et la sortie attendue (erreur). La r√©tropropagation utilise le gradient de l'erreur par rapport √† chaque poids, calcul√© via le calcul diff√©rentiel, pour mettre √† jour les poids dans la direction qui r√©duit l'erreur. Ce processus est r√©p√©t√© √† travers de multiples it√©rations ou √©poches d'entra√Ænement, permettant au r√©seau de devenir progressivement plus pr√©cis dans ses pr√©dictions.

Ce cadre fondamental des r√©seaux de neurones offre une flexibilit√© et une puissance de calcul qui ont r√©volutionn√© le TLP, permettant des avanc√©es significatives dans la compr√©hension et la g√©n√©ration du langage naturel.

## 3.2. R√©seaux de neurones r√©currents (RNN)

Les r√©seaux de neurones r√©currents (RNN) constituent une classe d'architectures de r√©seau de neurones sp√©cialement con√ßue pour traiter des s√©quences de donn√©es, ce qui les rend particuli√®rement adapt√©s aux applications de traitement du langage naturel (TLP). 

### Particularit√©s et utilisation des RNN en TLP

- **Particularit√©s des RNN** : La caract√©ristique distinctive des RNN est leur capacit√© √† maintenir un √©tat (ou m√©moire) qui capture des informations sur ce qui a √©t√© trait√© jusqu'√† pr√©sent dans la s√©quence. Cela leur permet de traiter des s√©quences de donn√©es de longueur variable, en prenant en compte non seulement l'entr√©e actuelle mais aussi le contexte fourni par les entr√©es pr√©c√©dentes. Cette propri√©t√© est cruciale pour le TLP, o√π la compr√©hension du contexte et la coh√©rence sur de longs segments de texte sont essentielles.

- **Utilisation des RNN en TLP** : En TLP, les RNN sont utilis√©s pour une vari√©t√© de t√¢ches telles que la traduction automatique, la g√©n√©ration de texte, la reconnaissance vocale, et l'analyse des sentiments. Leur capacit√© √† g√©rer le contexte les rend particuli√®rement efficaces pour ces applications, o√π la signification d√©pend fortement de la s√©quence des mots.

### Probl√®mes des RNN et introduction des LSTM et GRU

- **Probl√®mes des RNN** : Malgr√© leur efficacit√©, les RNN standards font face √† des d√©fis significatifs, notamment le probl√®me de la disparition ou de l'explosion des gradients lors de l'apprentissage sur de longues s√©quences. Ces probl√®mes rendent difficile pour les RNN d'apprendre des d√©pendances √† long terme dans les donn√©es, limitant leur capacit√© √† traiter efficacement des s√©quences de texte longues ou complexes.

- **Introduction des LSTM et GRU** :
  - **LSTM (Long Short-Term Memory)** : Les LSTM sont une am√©lioration des RNN con√ßue pour pallier le probl√®me de la disparition des gradients. Ils introduisent une structure de cellule complexe avec des portes sp√©cifiques (portes d'entr√©e, de sortie et d'oubli) qui r√©gulent le flux d'informations, permettant au r√©seau de retenir et d'oublier des informations de mani√®re s√©lective. Cela les rend capables de capturer des d√©pendances √† long terme sans souffrir de la disparition des gradients.
  
  - **GRU (Gated Recurrent Unit)** : Les GRU simplifient l'architecture des LSTM tout en conservant une grande partie de leur capacit√© √† g√©rer des d√©pendances √† long terme. Avec seulement deux portes (portes de r√©initialisation et de mise √† jour), les GRU offrent une alternative plus simple et souvent plus efficace aux LSTM pour certaines t√¢ches.

Ces √©volutions des RNN, les LSTM et GRU, ont largement contribu√© √† surmonter les limitations initiales des RNN, permettant des avanc√©es significatives dans le traitement des s√©quences longues et complexes en TLP.

## 3.3. L'essor des architectures Transformer

L'arriv√©e des architectures Transformer a marqu√© un tournant dans le domaine de l'intelligence artificielle, en particulier dans le traitement du langage naturel (TLP). Ces mod√®les se distinguent par leur capacit√© exceptionnelle √† traiter de mani√®re efficace des s√©quences de donn√©es de grande taille, surpassant les architectures pr√©c√©dentes en termes de performance et de flexibilit√©.

- **Introduction aux Transformers et √† l'auto-attention** : Les Transformers reposent sur le m√©canisme d'auto-attention, qui permet au mod√®le d'√©valuer diff√©rentes parties d'une s√©quence d'entr√©e en fonction de leur pertinence pour chaque √©l√©ment de la s√©quence. Cette approche diff√®re radicalement des mod√®les RNN et LSTM en ce qu'elle ne traite pas les s√©quences de mani√®re s√©quentielle, mais √©value l'ensemble de la s√©quence simultan√©ment. Cela permet une parall√©lisation beaucoup plus efficace lors de l'entra√Ænement et am√©liore la capacit√© du mod√®le √† g√©rer des d√©pendances √† longue distance dans le texte.

- **BERT, GPT, et autres mod√®les bas√©s sur Transformer** : 
  - **BERT (Bidirectional Encoder Representations from Transformers)** : D√©velopp√© par Google, BERT a innov√© en appliquant le concept de lecture bidirectionnelle du texte, permettant au mod√®le de capturer le contexte √† la fois √† gauche et √† droite de chaque token dans une s√©quence. Cela a men√© √† des am√©liorations substantielles dans des t√¢ches telles que la compr√©hension de texte et la r√©ponse aux questions.
  
  - **GPT (Generative Pre-trained Transformer)** : Lanc√© par OpenAI, GPT se distingue par sa capacit√© √† g√©n√©rer du texte de mani√®re coh√©rente et contextuellement pertinente. En utilisant une approche de pr√©-entra√Ænement suivi d'un affinage sp√©cifique √† la t√¢che, GPT a √©tabli de nouveaux standards dans la g√©n√©ration de texte, la traduction automatique, et au-del√†.

Les architectures Transformer ont transform√© le paysage du TLP, ouvrant la voie √† des avanc√©es significatives dans la compr√©hension et la g√©n√©ration de langage naturel. Leur flexibilit√©, combin√©e √† leur performance exceptionnelle, continue de stimuler l'innovation, faisant des Transformers l'une des technologies les plus influentes dans le domaine de l'intelligence artificielle aujourd'hui.

## Activit√©s en classe
- Impl√©mentation et entra√Ænement d'un mod√®le LSTM pour la classification des sentiments.

## Devoir
- Lecture critique d'un article de recherche sur les architectures Transformer et leur impact sur le TLP.

--------------
# Glossaire
Comme toute nouvelle technologie l'IA vient avec sont lot d‚Äôacronymes dont voici les principaux.

- **Auto-attention** : Un m√©canisme au c≈ìur des architectures Transformer qui permet √† chaque position dans une s√©quence de tenir compte de toutes les positions dans la m√™me s√©quence pour encoder une repr√©sentation de cette s√©quence. L'auto-attention aide le mod√®le √† se concentrer sur les parties importantes de l'entr√©e pour effectuer une t√¢che sp√©cifique.

- **BERT (Bidirectional Encoder Representations from Transformers)** : Un mod√®le pr√©-entra√Æn√© sur un grand corpus de texte qui utilise l'architecture Transformer pour g√©n√©rer des repr√©sentations contextuelles des mots. BERT a √©t√© con√ßu pour pr√©-entra√Æner des repr√©sentations profondes bidirectionnelles en examinant le contexte des mots √† gauche et √† droite dans toutes les couches. En cons√©quence, le mod√®le pr√©-entra√Æn√© peut √™tre finement ajust√© avec juste une couche de sortie suppl√©mentaire pour cr√©er des mod√®les de pointe pour une large gamme de t√¢ches de TLP.

- **GPT (Generative Pre-trained Transformer)** : Une s√©rie de mod√®les de langage qui utilisent l'architecture Transformer pour produire du texte. GPT est pr√©-entra√Æn√© sur un grand corpus de texte et peut √™tre utilis√© pour g√©n√©rer des textes coh√©rents et pertinents sur divers sujets en compl√©tant une invite donn√©e.

- **IA** : Intelligence Artificielle - Le domaine de la science informatique qui se concentre sur la cr√©ation de syst√®mes capables de r√©aliser des t√¢ches qui n√©cessiteraient normalement l'intelligence humaine, telles que la prise de d√©cision, la reconnaissance de la parole, la traduction entre langues, et plus encore.

- **GRU (Gated Recurrent Unit)** : Similaire aux LSTM, mais avec une structure plus simple, ce qui peut r√©duire la complexit√© et le temps de calcul. Les GRU utilisent √©galement des portes pour contr√¥ler le flux d'informations, mais elles combinent la porte d'oubli et la porte d'entr√©e en une seule.

- **LLM** : Mod√®les de Langage √† Grande √âchelle - Des syst√®mes d'intelligence artificielle con√ßus pour comprendre, g√©n√©rer, et interagir en langage naturel √† une √©chelle massive. Ils sont entra√Æn√©s sur d'immenses corpus de texte pour apprendre une grande vari√©t√© de t√¢ches linguistiques.

- **LSTM (Long Short-Term Memory)** : Une am√©lioration des RNN traditionnels, con√ßue pour √©viter le probl√®me de disparition du gradient, en introduisant des structures appel√©es cellules qui permettent au r√©seau de retenir des informations sur une longue p√©riode.

- **n-gram** : Un n-gram est une s√©quence continue de n √©l√©ments (mots, lettres) d'un texte donn√© ou d'une parole. Les mod√®les bas√©s sur n-grams ont √©t√© parmi les premi√®res approches utilis√©es pour le traitement du langage naturel.

- **RNN (R√©seaux de Neurones R√©currents)** : Une classe de r√©seaux de neurones con√ßue pour traiter des s√©quences de donn√©es, telle que le texte ou les s√©ries temporelles. Les RNN utilisent leur √©tat interne (m√©moire) pour traiter les s√©quences de donn√©es, ce qui les rend id√©aux pour des t√¢ches de TLP telles que la pr√©diction de mots suivants ou la compr√©hension du langage.

- **Transformers** : Une architecture de mod√®le introduite dans le papier "Attention Is All You Need" pour le traitement du langage naturel et d'autres t√¢ches de s√©quence. Elle utilise le m√©canisme d'auto-attention pour capter les d√©pendances √† longue distance entre les mots dans une phrase, am√©liorant significativement la performance par rapport aux RNN et LSTM sur de nombreuses t√¢ches de TLP.

- **Word2Vec** : Un groupe de mod√®les li√©s qui sont utilis√©s pour produire des plongements de mots (word embeddings). Ces mod√®les sont capables de capturer le contexte d'un mot dans un document, sa signification s√©mantique et syntaxique, simplement √† partir du texte brut.

# Ressources
La recherche en France : [IA au Loria, laboratoire du CNRS](https://ia.loria.fr/portfolio/)
[LLM(ChatGPT) - D√©-coder les grands mod√®les de langage - Christophe Cerisara | Codeurs en Seine](https://youtu.be/GiEcNK3XA_o)

[Aper√ßu complet sur les LLM et l'ing√©nierie des prompts | Baamtu](https://youtu.be/MWotDXpD6SI?si=txySpWEWK4vQ2A6V)

[The Practical Guides for Large Language Models | LLMsPracticalGuide](https://github.com/Mooler0410/LLMsPracticalGuide)

https://machinelearningmastery.com

[Les √©l√®ves utilisent ChatGPT pour leurs devoirs et les enseignants utilisent ChatGPT pour les corriger, d'apr√®s des rapports | Developpez.com](https://intelligence-artificielle.developpez.com/actu/355048/Les-eleves-utilisent-ChatGPT-pour-leurs-devoirs-et-les-enseignants-utilisent-ChatGPT-pour-les-corriger-d-apres-des-rapports-qui-suscitent-des-comparaisons-avec-les-examens-ecrits-et-oraux/)  
  
[Comment utiliser un LLM open source ? | Octo.com](https://blog.octo.com/comment-utiliser-un-llm-open-source-1)  
  
[Les mod√®les de langage peuvent contenir des backdoors | Nextinpact](https://next.ink/123823/les-modeles-de-langage-peuvent-contenir-des-backdoors/)

[Tuner un LLM √† partir de la documentation d'un projet](https://youtu.be/Ivp5PGIbGMw)

[n8n - Workflow Automation](https://github.com/n8n-io)

[Unslow AI training & finetuning Get 30x faster with unsloth](https://unsloth.ai/)

[Votre LLM (ChatGPT-like) √† la maison et comment coder par dessus. | Korben](https://youtu.be/1aXPuFrPtr0)

Vlog sur le d√©veloppement avec l'IA
[Don‚Äôt Build AI Products The Way Everyone Else Is Doing It](https://www.youtube.com/watch?v=bRFLE9qi3t8)

Groq les puces d√©di√©es √† l'IA
LeMondeInformatique
[Groq d√©fie Nvidia avec ses acc√©l√©rateurs LPU - Le Monde Informatique](https://www.lemondeinformatique.fr/actualites/lire-groq-defie-nvidia-avec-ses-accelerateurs-lpu-92822.html)
[NVidia vient de se faire d√©tronn√© | Underscore](https://youtu.be/fvWZ2kjTo-Q)

Encore inconnu il y a un an, Groq est bien d√©cid√© √† surfer sur la vague IA (g√©n√©rative ou autre) avec sa plateforme de calcul LPU taill√©e pour l'inf√©rence (r√©pondre au prompt).

R√©ponse de NVIDIA est de se positionner tr√®s fortement sur l'apprentissage et le tuning [ Nvidia vient juste de r√©volutionner l'I.A ? | cocadmin](https://youtu.be/JLYUlRxp_Z8)

[Un √©quivalent de GitHub Copilot gratuit √† installer sur Visual Studio Code | Korben](https://youtu.be/6a5GHdoa8OM) [TabbyML](https://github.com/TabbyML/tabby) √âcrit en RUST, Permet d'installer un serveur conversationnel via docker. Le client pour visual studio code est disponible via les extensions en cherchant tabby. Tabby permet de g√©rer des √©quipes et poss√®de une API

Le futur de l'IDE **Cursor** [Using Cursor - the AI powered VS Code alt for the first time‚Ä¶| Huw prosser](https://youtu.be/n4DRPGWTmpc)

Agr√©gation de textes du domaine public pour l'entrainement [common corpus des textes du domaine public pour entrainer des IA generatives](https://next.ink/131929/common-corpus-des-textes-du-domaine-public-pour-entrainer-des-ia-generatives/)

# Les LLM en bref

[r√©sum√© de la conf√©rence : LLM(ChatGPT) - D√©-coder les grands mod√®les de langage - Christophe Cerisara | Codeurs en Seine](https://youtu.be/GiEcNK3XA_o)

Sur Hugging Face, il y a plus de 70 000 mod√®les open source [Huggingface/models](https://huggingface.co/models).
Aujourd'hui, l'apprentissage d'un LLM se fait sur 1 000 000 000 000 de mots. L'architecture des transformers (2018 Google) est celle de la quasi-totalit√© des LLM ; elle vise √† pr√©dire le mot suivant.

Les LLM sont tr√®s bons pour √©crire du code simple ; faire du code compliqu√© n√©cessite de le guider et donc d'avoir une interaction avec.

Pour g√©rer la complexit√©, il y a des tentatives d'agents de mod√®les de langage (orchestration de diff√©rents LLMs) (Langchain, Coala).

Aujourd'hui, on constate les capacit√©s des LLMs mais on commence √† peine √† pouvoir les expliquer (presque pas, embryons de th√©ories).
Les observations :
- M√©morisation, compression, structuration, et g√©n√©ralisation.
- Capacit√©s √©mergentes :
	- "In-context learning" : Capacit√© de g√©n√©raliser (seuls les LLM font cela), mais n'√©merge qu'√† partir de 10^10 de param√®tres.
	- Capacit√© de faire des additions de 3 chiffres (10^10).
	- R√©pondre √† des questions.
	- G√©n√©rer des programmes.
	- Jason Wei a d√©nombr√© 137 capacit√©s √©mergentes :
		- D√©composition d'un probl√®me en √©tapes.
		- "Prompt of thought" (Acc√©der au raisonnement du LLM).
		- "Analogical prompting"  (Faire √©laborer au LLM la d√©marche √† appliquer avant de l'appliquer).
		- Instruction proc√©durale.
		- Anagramme.
		- Arithm√©tique modulaire.
		- Probl√®mes simples de math√©matiques.
		- D√©duction logique.
		- D√©duction analytique.
		- Th√©orie de l'esprit (avoir un mod√®le de son locuteur) ?
		- ...
Embryons de th√©ories **Passage √† l'√©chelle** ("scaling law") :
	- Lois de Chinchilla (On mesure les capacit√©s du mod√®le lorsqu'il devient de plus en plus gros. Baidu 2017 puis Google 2018 : l'apprentissage suit des lois qui sont fonction du nombre de donn√©es, et aboutit en 2022 √† Chinchilla ; ces lois sont en `L(N)=A/N**ùõº` o√π ùõº est diff√©rent pour chacune des architectures, avec le meilleur score de 0,54 pour les "transformers").
	- Grokking (Apprend bestialement puis, apr√®s beaucoup de donn√©es, g√©n√©ralise).
	- Double descente.
	- Transition de phases.
