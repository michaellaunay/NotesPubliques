
[L'utopie de la Modélisation, première partie, qu'est ce qu'un modèle ?](https://www.linkedin.com/pulse/lutopie-de-la-mod%2525C3%2525A9lisation-premi%2525C3%2525A8re-partie-quest-ce-quun-launay)

Nos sociétés ont toujours cherché à expliquer et prédire ce qui leur arrive.

Depuis le siècle des Lumières, les mathématiques sont devenues l’outil le plus efficace de prédiction et les « scientifiques » ont remplacé les augures.

L’une des conséquences les plus importantes a été l’usage des statistiques qui permirent aux politiques de prendre "de bonnes" décisions. Il est ainsi devenu possible de passer de l’expérience individuelle à celle du groupe et de remplacer la diseuse de bonnes aventures par le pourcentage. La place sociale de la mort a été profondément modifiée et nous sommes passés du fait divin imprévisible et incompréhensible à la certitude qu'un certain pourcentage des individus vivront et auront des enfants et qu’il est possible de prévoir au plus juste les infrastructures, et les dépenses nécessaires.

Le bon souverain devint alors un bon gestionnaire. Puis les états utilisèrent des modèles de leur fonctionnement et pûrent ainsi assurer leur pérennité. Bref, nous étions passés d’un modèle purement « historique », où la connaissance des faits et actions permettait de se projeter dans un monde constant, à un modèle statistique de notre société permettant de prévoir les évolutions « linéaires », de les anticiper voire de les favoriser ou non.

Pour la balistique, nos « scientifiques » ont développé les systèmes d’équations différentielles linéaires qui par résolution permettaient de prévoir la bonne trajectoire de nos projectiles, mais nécessitaient un calcul rapide. La guerre devint une science gourmande en technique.

Les puissances de calcul ont alors augmenté en même temps que les besoins, et la technologie a créé de nouveaux besoins avec à la clé une croissance exponentielle de nos savoirs et de nos économies. Mais fin 19ème, il fallait plus de 15 ans pour passer du recensement d'un grand pays aux chiffres permettant les projections utiles aux décisions politiques. IBM naquit des premières machines mécanographiques de Hermann Hollerith, qui en utilisant des cartes perforées réalisées pendant les recensements permettaient d’avoir les statistiques nécessaires aux décisions politiques en moins de trois ans ( https://interstices.info/linvention-de-la-mecanographie/ ).

Cela a si bien fonctionné que ces outils ont été utilisés par les dictatures du 20ème siècle pour mettre en place l’élimination de masse. Mais il faut comprendre que la rationalisation et la création de modèles avaient enfanté du pire, et le nazisme n'est pas seulement la pensée monstrueuse de certains, mais bien le produit d'une intense réflexion autour de modèles de société, je vous invite à ce propos à voir les conférences de Johann Chapoutot (https://youtu.be/Lfe5shsp4Hw ) sur le sujet.

Depuis les modèles sont partout et touchent tous les domaines d'activité humaine, ils sont par nature descriptifs et prédictifs et sont plus simples à appréhender que la réalité, mais ne sont pas toujours complets et ont généralement des limites d'applications.

Un bon modèle est simple et précis, manipulable et permet d’avoir des prévisions suffisamment proches de la réalité pour présenter un intérêt.

Le langage de description d'un modèle dépend de son domaine d’application, il peut prendre la forme d’équations mathématiques linéaires, stochastiques, de systèmes bouclés dit à rétroaction https://fr.wikipedia.org/wiki/R%C3%A9troaction, d'algorithmes sophistiqués permettant de modéliser des comportements discontinus et hybrides ( https://fr.wikipedia.org/wiki/R%C3%A9troaction ), ou celle de descriptions littéraires qui généralement finissent par être traduite en équation.

On trouve des modélisations numériques de cellule vivante (https://www.pourlascience.fr/sd/biologie-cellulaire/simuler-une-cellule-vivante-7939.php ) à des descriptions textuelles et factuelles d’une bataille historique. Il est d’ailleurs étonnant de voir comment un même modèle peut être utilisé pour des domaines différents, par exemple les boucles de rétroaction des modèles en physique de systèmes oscillants de l’électronique peuvent être utilisées en science du vivant pour élaborer des relations entre lapins et carottes ou entre hommes et consommation des ressources primaires. La grande difficulté et non des moindres et alors d’identifier les similitudes et de savoir faire les transformations d’un modèle vers un autre et ainsi de remonter dans une science les trouvailles faites par une autre science.

Mais le plus beau est que notre cerveau crée des modèles sans nécessairement intellectualiser la démarche, le résultat est ce que nous nommons avoir de l'expérience. Le cerveau le fait en permanence en réarrangeant continuellement nos connexions synaptiques. La modélisation de ce fonctionnement est à l'origine des réseaux de neurones utilisés en informatique, qui a poursuivi son propre développement pour aboutir à l'apprentissage profond (deep learning) où comme pour l'humain, c'est le nombre, la qualité et la variabilité des exemples utilisés lors de l'apprentissage qui font la qualité de la cognition (je vous invite à suivre la prochaine session du Mooc deep learning présenté par le CNAM https://www.fun-mooc.fr/courses/course-v1:CNAM+01031+session02/about# ).

Toutefois, notre cerveau n’hésite pas à compléter les informations perçues, il comble les trous de notre perception visuelle, prend des raccourcis pour traiter plus rapidement l’information reçue. Ces optimisations sont efficaces pour la majorité des cas, mais créer des biais cognitifs exploitables, comme ceux liés à la perception du mouvement très utilisés par les magiciens, ou ceux sociaux comme les biais de confiance ou de renforcement  (voir l'excellente série "Crétin de cerveau" de la chaîne ScienceEtonnante https://youtu.be/xJO5GstqTSY ), etc.

Mais en fait l'élaboration de modèle tout comme l'apprentissage ne sont que quêtes incessantes. Chaque fois que quelqu’un affirme avoir « enfin » un modèle complet vient une mesure ou expérience entrant en contradiction avec les prédictions faites, et l'on s’aperçoit alors qu’il faut travailler à compléter voire redéfinir le modèle.

Le modèle initial perd alors de sa simplicité, comme dans le cas des lois de Newton qui à cause de la planète Mercure ont été "complétées" par les équations d’Einstein  (voir "Einstein et la pensée de Newton" de  https://halshs.archives-ouvertes.fr/halshs-00177336/document )

Mais il arrive qu’on doive renoncer à atteindre un modèle complet. Ainsi, comme en mathématiques avec le théorème d’incomplétude de Gohdel ( https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8mes_d%27incompl%C3%A9tude_de_G%C3%B6del ), il arrive qu'on rencontre des problèmes qui nécessitent de modifier le système de représentation du modèle, car il n'y a pas de concept dans le langage de modélisation permettant de le décrire et que le modèle porte des concepts modifiant le langage de description du modèle. Bref il apparaît des problèmes indécidables nécessitant une extension profonde du langage utilisé pour la modélisation, à ne pas confondre avec les DSL (Domain Specific Language) qui reviennent à regrouper l'assemblage de plusieurs concepts sous le nom d'un nouveau concept afin "d'aérer" le modèle ( https://fr.wikipedia.org/wiki/Langage_d%C3%A9di%C3%A9 ) .

On peut parler de classe de modèle, c’est-à-dire que les modèles peuvent être rangés par catégorie selon leur structure. Dans une classe les modèles sont substituables alors qu’ils ne traitent pas du même domaine, on parle de généralisation ou « pattern » de modèle. En informatique, il a ainsi été identifié des motifs récurrents dans les conceptions logicielles qui fonctionnent et ces motifs sont devenus des patrons de conceptions https://fr.wikipedia.org/wiki/Patron_de_conception .

À force d'abstraction, on finit par perdre « le principe de réalité », ainsi :

La plupart des modèles des sciences « pures » sont indépendants de la réalité, bref si on modifie le modèle, la réalité ne change pas. La démarche scientifique consiste à élaborer des modèles que l’on teste pour prédire la réalité et que l’on modifie itérativement jusqu’à converger vers un modèle le plus simple possible et le plus précis possible, généralement jusqu’à ce que notre connaissance globale ait suffisamment augmenté pour pouvoir le compléter. À titre d’exemple les médecins grecs d’Alexandrie dont Èrasistrate ( https://fr.wikipedia.org/wiki/%C3%89rasistrate ) avait établi un modèle "pneumatique" du corps, notamment pour l'activation des muscles. Ils ne comprenaient pas  l’électricité et son rôle dans le contrôle des muscles qui furent découverts par Luigi Galvani ( https://fr.wikipedia.org/wiki/Luigi_Galvani ). Ils ne pouvaient donc imaginer d’autres interactions et il aura fallu attendre 20 siècles que l’ensemble de la société ait absorbé le niveau scientifique suffisant pour qu’apparaisse la notion d’influx électrique comme déclencheur de l’action musculaire. Les découvertes faites à une époque sont corrélées avec le niveau culturel de la population, plus il y a de sachants, plus la chance pour que l’on trouve et que cela se diffuse est importante. Par exemple l’explication du rôle des artères qui commence à l'époque des grandes dissections de Léonard de Vinci et André Vésale, mettra près de 300 ans à s'imposer ( https://www.pourlascience.fr/sd/medecine/le-combat-de-la-circulation-sanguine-5133.php ).

Ainsi, et encore aujourd'hui, même si l'information circule à la vitesse de la lumière, tout "nouveau" modèle demande du temps pour être testé (principe de réfutabilité). Une fois validé, et son contexte d'application déterminé, le modèle aura besoin de temps pour imprégner notre culture, et dans certains cas changer nos éléments de langage. Par exemple en 1905, Einstein met à terre notre conception d'un temps absolu et l'on peut constater que malgré les preuves expérimentales accumulées démontrant un temps local, nous parlons toujours d'un temps qui coule, unique à tous ( voir les conférences d'Étienne Klein sur le sujet https://youtu.be/FGyh7BmIzrM ). Bref, nous continuons à raisonner avec nos vieux réflexes. Pire, ces biais culturels nous freinent dans la compréhension de nouveau modèle.

Dans d’autres sciences, les modèles ont un pouvoir de transformation du réel, proportionnel à la croyance dans le modèle choisi, ainsi en économie le modèle prédominant oriente les politiques et modifie la réalité, quitte à devoir appliquer une énergie très importante pour tenter de conformer la réalité mesurée au modèle. Nous pouvons dire qu’au plus le comportement d’un agent s’écarte du modèle, au plus le groupe appliquera une force de rappel importante pour ramener l'égaré vers le chemin tracé par le modèle.

On retrouve aussi cela en politique, mais là la vertu du marketing et de la communication transforme le modèle en conviction marketée pour être facilement ingérable par de nouveaux "croyants". Les conséquences frôlent parfois la catastrophe, comme dans le cas de la société d'investissements en bourse LTCM dont le modèle établi par trois prix Nobel a failli conduire à une catastrophe majeure au système bancaire ( https://fr.wikipedia.org/wiki/Long_Term_Capital_Management )


Ma société Ecréall travaille sur les processus métiers.

En effet, tous les managers rêvent d’avoir un modèle de leurs équipes, de leur métier, de pouvoir générer toute la chaîne de production à partir de leurs modèles et inversement de pouvoir modéliser le réel pour mieux le comprendre.

La première difficulté est de posséder un embryon de notation pour faire cela, puis un outil de modélisation, une chaîne de transformation pour « physicaliser » le modèle et enfin une boucle de rétroaction pour modifier le modèle pour la faire correspondre à la réalité et/ou l’inverse selon le désir du manager.

Mais cela ne marche pas ainsi, car chaque agent d’un système a une vision partielle et partisane du processus métier. On doit alors interroger chaque agent et comme dans une véritable enquête reconstituer le processus métier à partir des traces laisser par les agents et/ou à partir de leur déclaration.

Le Graal étant bien sûr de pouvoir optimiser la chaîne depuis le modèle.

Amen Souissi l’un des associés a réalisé sa thèse de doctorat sur le sujet « Modélisation de processus métier et génération de portail collaboratif » ( https://tel.archives-ouvertes.fr/tel-00935324/ ). Amen a ainsi démontré la faisabilité de cette approche. Il a pu modéliser un processus d’idéation et a généré la première version de notre boîte à idées collaborative.


Je détaillerai, dans le prochain article, l'expérience accumulée et les résultats obtenus.